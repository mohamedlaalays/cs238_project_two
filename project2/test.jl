struct GradientQLearning
    ğ’œ # action space (assumes 1:nactions)
    Î³ # discount
    Q # parameterized action value function Q(Î¸,s,a) âˆ‡Q # gradient of action value function
    Î¸ # action value function parameter
    Î± # learning rate
end
function lookahead(model::GradientQLearning, s, a)
    return model.Q(model.Î¸, s,a)
end 

function update!(model::GradientQLearning, s, a, r, sâ€²)
    ğ’œ, Î³, Q, Î¸, Î± = model.ğ’œ, model.Î³, model.Q, model.Î¸, model.Î± u = maximum(Q(Î¸,sâ€²,aâ€²) for aâ€² in ğ’œ)
    Î” = (r + Î³*u - Q(Î¸,s,a))*model.âˆ‡Q(Î¸,s,a)
    Î¸[:] += Î±*scale_gradient(Î”, 1)
    return model
end 



Î²(s,a) = [s,s^2,a,a^2,1]
Q(Î¸,s,a) = dot(Î¸,Î²(s,a))
âˆ‡Q(Î¸,s,a) = Î²(s,a)
Î¸ = [0.1,0.2,0.3,0.4,0.5] # initial parameter vector Î± = 0.5 # learning rate
model = GradientQLearning(ğ’«.ğ’œ, ğ’«.Î³, Q, âˆ‡Q, Î¸, Î±) Îµ = 0.1 # probability of random action
Ï€ = EpsilonGreedyExploration(Îµ)
k = 20 # number of steps to simulate
s = 0.0 # initial state simulate(ğ’«, model, Ï€, k, s)



mutable struct LocallyWeightedValueFunction 
    k # kernel function k(s, sâ€²)
    S # set of discrete states
    Î¸ # vector of values at states in S
end 
function (UÎ¸::LocallyWeightedValueFunction)(s)
    w = normalize([UÎ¸.k(s,sâ€²) for sâ€² in UÎ¸.S], 1)
    return UÎ¸.Î¸ â‹… w
end
function fit!(UÎ¸::LocallyWeightedValueFunction, S, U) 
    UÎ¸.Î¸ = U
    return UÎ¸
end 